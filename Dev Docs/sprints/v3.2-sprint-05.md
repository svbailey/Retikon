# V3.2 Sprint 05 - Retrieval quality + evaluation

Dates: TBD
Owner: Search/IR

## Goal
Improve retrieval quality and add evaluation loops.

## Scope checklist (Core)
- [ ] Hybrid retrieval (keyword + vector fusion).
- [ ] Reranking stage with configurable reranker.
- [ ] Eval harness + CI regression suite (Ragas + Phoenix).
- [ ] Basic feedback capture (thumbs up/down).

## Scope checklist (Pro)
- [ ] None

## Primary paths (Core)
- `retikon_core/query_engine/`
- `retikon_core/eval/` (new)
- `retikon_core/feedback/` (new)

## Tests and validation
- [ ] Core: `tests/core/test_retrieval_hybrid.py` (new)
- [ ] Core: `tests/core/test_reranking.py` (new)
- [ ] Core: `tests/core/test_eval_harness.py` (new)

## Deliverables
- [ ] Reranking improves top-5 relevance on eval set
- [ ] Evaluation harness runs in CI

## Risks / Dependencies
- [ ] Eval datasets availability

## Notes
-
